{"cells":[{"source":"# Practical Exam: Supermarket Loyalty\n\nInternational Essentials is an international supermarket chain.\n\nShoppers at their supermarkets can sign up for a loyalty program that provides rewards each year to customers based on their spending. The more you spend the bigger the rewards. \n\nThe supermarket would like to be able to predict the likely amount customers in the program will spend, so they can estimate the cost of the rewards. \n\nThis will help them to predict the likely profit at the end of the year.\n\n## Data\n\nThe dataset contains records of customers for their last full year of the loyalty program.\n\n| Column Name | Criteria                                                |\n|-------------|---------------------------------------------------------|\n|customer_id | Unique identifier for the customer. </br>Missing values are not possible due to the database structure. |\n|spend | Continuous. </br>The total spend of the customer in their last full year. This can be any positive value to two decimal places. </br>Missing values should be replaced with 0. |\n|first_month | Continuous. </br>The amount spent by the customer in their first month of the year. This can be any positive value, rounded to two decimal places. </br>Missing values should be replaced with 0. |\n| items_in_first_month | Discrete. </br>The number of items purchased in the first month. Any integer value greater than or equal to zero. </br>Missing values should be replaced by 0. |  \n| region | Nominal. </br>The geographic region that the customer is based in. One of four values Americas, Asia/Pacific, Europe, Middle East/Africa. </br>Missing values should be replaced with \"Unknown\". |\n| loyalty_years | Oridinal. </br>The number of years the customer has been a part of the loyalty program. One of five ordered categories, '0-1', '1-3', '3-5', '5-10', '10+'. </br>Missing values should be replaced with '0-1'.|\n| joining_month | Nominal. </br>The month the customer joined the loyalty program. One of 12 values \"Jan\", \"Feb\", \"Mar\", \"Apr\", etc. </br>Missing values should be replaced with \"Unknown\".|\n| promotion | Nominal. </br>Did the customer join the loyalty program as part of a promotion? Either 'Yes' or 'No'. </br>Missing values should be replaced with 'No'.|\n","metadata":{},"id":"0a8ca74a-b235-4034-9c1c-3159336a39d5","cell_type":"markdown"},{"source":"# Task 1\n\nBefore you fit any models, you will need to make sure the data is clean. \n\nThe table below shows what the data should look like. \n\nCreate a cleaned version of the dataframe. \n\n - You should start with the data in the file \"loyalty.csv\". \n\n - Your output should be a dataframe named `clean_data`. \n\n - All column names and values should match the table below.\n\n| Column Name | Criteria                                                |\n|-------------|---------------------------------------------------------|\n|customer_id | Unique identifier for the customer. </br>Missing values are not possible due to the database structure. |\n|spend | Continuous. </br>The total spend of the customer in their last full year. This can be any positive value to two decimal places. </br>Missing values should be replaced with 0. |\n|first_month | Continuous. </br>The amount spent by the customer in their first month of the year. This can be any positive value, rounded to two decimal places. </br>Missing values should be replaced with 0. |\n| items_in_first_month | Discrete. </br>The number of items purchased in the first month. Any integer value greater than or equal to zero. </br>Missing values should be replaced by 0. |  \n| region | Nominal. </br>The geographic region that the customer is based in. One of four values Americas, Asia/Pacific, Europe, Middle East/Africa. </br>Missing values should be replaced with \"Unknown\". |\n| loyalty_years | Oridinal. </br>The number of years the customer has been a part of the loyalty program. One of five ordered categories, '0-1', '1-3', '3-5', '5-10', '10+'. </br>Missing values should be replaced with '0-1'.|\n| joining_month | Nominal. </br>The month the customer joined the loyalty program. One of 12 values \"Jan\", \"Feb\", \"Mar\", \"Apr\", etc. </br>Missing values should be replaced with \"Unknown\".|\n| promotion | Nominal. </br>Did the customer join the loyalty program as part of a promotion? Either 'Yes' or 'No'. </br>Missing values should be replaced with 'No'.|","metadata":{},"id":"790f8bb8-76fb-44fd-8078-c62909a91b2b","cell_type":"markdown"},{"source":"import pandas as pd\n\n# Load the data\ndf = pd.read_csv(\"loyalty.csv\")\n\n# Task 1: Identify and replace missing values\n\n# Replace missing values in 'spend' with 0\ndf['spend'].fillna(0, inplace=True)\ndf['spend'] = df['spend'].astype(float)  # Ensure 'spend' is a float\n\n# Replace missing values in 'first_month' with 0\ndf['first_month'] = df['first_month'].apply(lambda x: float(x) if str(x).replace('.', '', 1).isdigit() else 0)\n# Ensure all values are positive and rounded to two decimal places\ndf['first_month'] = df['first_month'].apply(lambda x: round(max(x, 0), 2))\ndf['first_month'] = df['first_month'].astype(float)\n\n# Replace missing values in 'items_in_first_month' with 0\ndf['items_in_first_month'].fillna(0, inplace=True)\ndf['items_in_first_month'] = df['items_in_first_month'].astype(int)  # Ensure 'items_in_first_month' is an integer\n\n# Replace missing values in 'region' with 'Unknown'\ndf['region'].fillna('Unknown', inplace=True)\ndf['region'] = df['region'].astype(str)  # Ensure 'region' is a string\n\n# Replace missing values in 'loyalty_years' with '0-1'\ndf['loyalty_years'].fillna('0-1', inplace=True)\ndf['loyalty_years'] = df['loyalty_years'].astype(str)  # Ensure 'loyalty_years' is a string\n\n# Replace missing values in 'joining_month' with 'Unknown'\ndf['joining_month'].fillna('Unknown', inplace=True)\ndf['joining_month'] = df['joining_month'].astype(str)  # Ensure 'joining_month' is a string\n\n# Replace missing values in 'promotion' with 'No'\ndf['promotion'].fillna('No', inplace=True)\ndf['promotion'] = df['promotion'].astype(str)  # Ensure 'promotion' is a string\n\n# Task 2: Clean categorical and text data by manipulating strings\n\n# Clean 'region' column\ndf['region'] = df['region'].replace({\n    'America': 'Americas',  # Fix potential misspelling\n    'Asia': 'Asia/Pacific',  # Fix potential misspelling\n    'Europe': 'Europe',\n    'Middle East': 'Middle East/Africa'\n})\n\n# Clean 'loyalty_years' column\ndf['loyalty_years'] = df['loyalty_years'].replace({\n    '1-2': '1-3',  # Standardize ranges\n    '2-3': '3-5'\n})\n\n# Clean 'joining_month' column\ndf['joining_month'] = df['joining_month'].replace({\n    'Janury': 'Jan',  # Fix potential misspelling\n    'Febuary': 'Feb'\n})\n\n# Clean 'promotion' column\ndf['promotion'] = df['promotion'].str.strip().str.title()  # Capitalize 'Yes'/'No'\n\n# Ensure correct data types\ndf['spend'] = df['spend'].round(2)  # Round 'spend' to two decimal places\ndf['first_month'] = df['first_month'].round(2)  # Round 'first_month' to two decimal places\n\n# Create cleaned dataframe\nclean_data = df\n\n# Save cleaned data to a new CSV file if needed\n# clean_data.to_csv(\"cleaned_loyalty.csv\", index=False)\n\n# Display the cleaned dataframe\nprint(clean_data.dtypes)\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"visualizeDataframe":false,"chartConfig":{"bar":{"hasRoundedCorners":true,"stacked":false},"type":"bar","version":"v1"},"outputsMetadata":{"0":{"height":395,"type":"stream"},"1":{"height":338,"type":"dataFrame"}}},"id":"523dd03c-8591-4cc6-b750-d71da287745f","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"customer_id               int64\nspend                   float64\nfirst_month             float64\nitems_in_first_month      int64\nregion                   object\nloyalty_years            object\njoining_month            object\npromotion                object\ndtype: object\ncustomer_id               int64\nspend                   float64\nfirst_month             float64\nitems_in_first_month      int64\nregion                   object\nloyalty_years            object\njoining_month            object\npromotion                object\ndtype: object\n"}]},{"source":"# Task 2 \n\nThe team at International Essentials have told you that they have always believed that the number of years in the loyalty scheme is the biggest driver of spend. \n\nProducing a table showing the difference in the average spend by number of years in the loyalty programme along with the variance to investigate this question for the team.\n\n - You should start with the data in the file 'loyalty.csv'.\n\n - Your output should be a data frame named `spend_by_years`. \n\n - It should include the three columns `loyalty_years`, `avg_spend`, `var_spend`. \n\n - Your answers should be rounded to 2 decimal places.   ","metadata":{},"id":"08b695b7-67db-48fb-8b14-e12bb5a9620e","cell_type":"markdown"},{"source":"# Use this cell to write your code for Task 2\nimport pandas as pd\n\ndf = pd.read_csv('loyalty.csv')\nprint(df.head(5))\n\n# Group by 'loyalty_years' and calculate the mean and variance of 'spend'\nspend_by_years = df.groupby('loyalty_years')['spend'].agg(['mean', 'var']).reset_index()\n\n# Rename columns as required\nspend_by_years.columns = ['loyalty_years', 'avg_spend', 'var_spend']\n\n# Round the values to 2 decimal places\nspend_by_years = spend_by_years.round(2)\n\n# Display the resulting dataframe\nspend_by_years\n\n","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1722501592501,"lastExecutedByKernel":"1c310bdc-89fd-433b-82b8-575b9c2ed850","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Use this cell to write your code for Task 2\nimport pandas as pd\n\ndf = pd.read_csv('loyalty.csv')\nprint(df.head(5))\n\n# Group by 'loyalty_years' and calculate the mean and variance of 'spend'\nspend_by_years = df.groupby('loyalty_years')['spend'].agg(['mean', 'var']).reset_index()\n\n# Rename columns as required\nspend_by_years.columns = ['loyalty_years', 'avg_spend', 'var_spend']\n\n# Round the values to 2 decimal places\nspend_by_years = spend_by_years.round(2)\n\n# Display the resulting dataframe\nspend_by_years\n\n","outputsMetadata":{"0":{"height":185,"type":"stream"},"1":{"height":196,"type":"dataFrame"}}},"id":"cc590298-c483-4253-bef1-a352933cbd5e","cell_type":"code","execution_count":213,"outputs":[{"output_type":"stream","name":"stdout","text":"   customer_id   spend first_month  ...  loyalty_years joining_month promotion\n0            1  132.68        15.3  ...           5-10           Nov        No\n1            2  106.45        16.2  ...            0-1           Feb       Yes\n2            3  123.16       25.26  ...            10+           Dec       Yes\n3            4  130.60       24.74  ...            3-5           Apr        No\n4            5  130.41       25.59  ...            3-5           Apr       Yes\n\n[5 rows x 8 columns]\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"loyalty_years","type":"string"},{"name":"avg_spend","type":"number"},{"name":"var_spend","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"loyalty_years":["0-1","1-3","10+","3-5","5-10"],"avg_spend":[110.56,129.31,117.41,124.55,135.15],"var_spend":[9.3,9.65,16.72,11.09,14.1]}},"total_rows":5,"truncation_type":null},"text/plain":"  loyalty_years  avg_spend  var_spend\n0           0-1     110.56       9.30\n1           1-3     129.31       9.65\n2           10+     117.41      16.72\n3           3-5     124.55      11.09\n4          5-10     135.15      14.10","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loyalty_years</th>\n      <th>avg_spend</th>\n      <th>var_spend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0-1</td>\n      <td>110.56</td>\n      <td>9.30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1-3</td>\n      <td>129.31</td>\n      <td>9.65</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10+</td>\n      <td>117.41</td>\n      <td>16.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3-5</td>\n      <td>124.55</td>\n      <td>11.09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5-10</td>\n      <td>135.15</td>\n      <td>14.10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":213}]},{"source":"# Task 3\n\nFit a baseline model to predict the spend over the year for each customer.\n\n 1. Fit your model using the data contained in “train.csv” </br></br>\n\n 2. Use “test.csv” to predict new values based on your model. You must return a dataframe named `base_result`, that includes `customer_id` and `spend`. The `spend` column must be your predicted values.","metadata":{},"id":"7113acde-8a74-487a-8983-f0c93003d945","cell_type":"markdown"},{"source":"import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Load training and test data\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n\n# Define features and target\nfeatures = ['first_month', 'items_in_first_month', 'region', 'loyalty_years', 'joining_month']\ntarget = 'spend'\n\n# Separate features and target in training data\nX_train = train_df[features]\ny_train = train_df[target]\n\n# Preprocess data\n# Define a preprocessing pipeline for categorical and numerical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='mean'), ['first_month', 'items_in_first_month']),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), ['region', 'loyalty_years', 'joining_month'])\n    ])\n\n# Create a pipeline that first preprocesses the data and then fits the model\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Prepare the test data\nX_test = test_df[features]\n\n# Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Create result dataframe\nbase_result = pd.DataFrame({\n    'customer_id': test_df['customer_id'],\n    'spend': predictions\n})\n\n# Save the result to a CSV file if needed\n# base_result.to_csv('base_result.csv', index=False)\n\n# Display the result dataframe\nprint(base_result.head())\n","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1722501592553,"lastExecutedByKernel":"1c310bdc-89fd-433b-82b8-575b9c2ed850","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Load training and test data\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n\n# Define features and target\nfeatures = ['first_month', 'items_in_first_month', 'region', 'loyalty_years', 'joining_month']\ntarget = 'spend'\n\n# Separate features and target in training data\nX_train = train_df[features]\ny_train = train_df[target]\n\n# Preprocess data\n# Define a preprocessing pipeline for categorical and numerical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='mean'), ['first_month', 'items_in_first_month']),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), ['region', 'loyalty_years', 'joining_month'])\n    ])\n\n# Create a pipeline that first preprocesses the data and then fits the model\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Prepare the test data\nX_test = test_df[features]\n\n# Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Create result dataframe\nbase_result = pd.DataFrame({\n    'customer_id': test_df['customer_id'],\n    'spend': predictions\n})\n\n# Save the result to a CSV file if needed\n# base_result.to_csv('base_result.csv', index=False)\n\n# Display the result dataframe\nprint(base_result.head())\n","visualizeDataframe":false,"chartConfig":{"bar":{"hasRoundedCorners":true,"stacked":false},"type":"bar","version":"v1"},"outputsMetadata":{"0":{"height":143,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"79a77f11-b09b-4d70-893b-535dfde919b2","cell_type":"code","execution_count":214,"outputs":[{"output_type":"stream","name":"stdout","text":"   customer_id       spend\n0            5  140.710078\n1            7  148.726371\n2           16  140.820811\n3           17  150.640023\n4           19  153.637394\n"}]},{"source":"# Task 4\n\nFit a comparison model to predict the spend over the year for each customer.\n\n 1. Fit your model using the data contained in “train.csv” </br></br>\n\n 2. Use “test.csv” to predict new values based on your model. You must return a dataframe named `compare_result`, that includes `customer_id` and `spend`. The `spend` column must be your predicted values.","metadata":{},"id":"44033abf-a603-479e-8663-9a96fefee5a2","cell_type":"markdown"},{"source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Load training and test data\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n\n# Define features and target\nfeatures = ['first_month', 'items_in_first_month', 'region', 'loyalty_years', 'joining_month']\ntarget = 'spend'\n\n# Separate features and target in training data\nX_train = train_df[features]\ny_train = train_df[target]\n\n# Preprocess data\n# Define a preprocessing pipeline for categorical and numerical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='mean'), ['first_month', 'items_in_first_month']),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), ['region', 'loyalty_years', 'joining_month'])\n    ])\n\n# Create a pipeline that first preprocesses the data and then fits the model\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n])\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Prepare the test data\nX_test = test_df[features]\n\n# Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Create result dataframe\ncompare_result = pd.DataFrame({\n    'customer_id': test_df['customer_id'],\n    'spend': predictions\n})\n\n# Save the result to a CSV file if needed\n# compare_result.to_csv('compare_result.csv', index=False)\n\n# Display the result dataframe\nprint(compare_result.head())\n","metadata":{"executionCancelledAt":null,"executionTime":485,"lastExecutedAt":1722501593038,"lastExecutedByKernel":"1c310bdc-89fd-433b-82b8-575b9c2ed850","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Load training and test data\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n\n# Define features and target\nfeatures = ['first_month', 'items_in_first_month', 'region', 'loyalty_years', 'joining_month']\ntarget = 'spend'\n\n# Separate features and target in training data\nX_train = train_df[features]\ny_train = train_df[target]\n\n# Preprocess data\n# Define a preprocessing pipeline for categorical and numerical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='mean'), ['first_month', 'items_in_first_month']),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), ['region', 'loyalty_years', 'joining_month'])\n    ])\n\n# Create a pipeline that first preprocesses the data and then fits the model\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n])\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Prepare the test data\nX_test = test_df[features]\n\n# Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Create result dataframe\ncompare_result = pd.DataFrame({\n    'customer_id': test_df['customer_id'],\n    'spend': predictions\n})\n\n# Save the result to a CSV file if needed\n# compare_result.to_csv('compare_result.csv', index=False)\n\n# Display the result dataframe\nprint(compare_result.head())\n","outputsMetadata":{"0":{"height":143,"type":"stream"}}},"id":"731cfa01-2709-413a-ac19-611e73e37c75","cell_type":"code","execution_count":215,"outputs":[{"output_type":"stream","name":"stdout","text":"   customer_id     spend\n0            5  140.5791\n1            7  148.8705\n2           16  141.1345\n3           17  150.7497\n4           19  153.6241\n"}]},{"source":"# Use this cell to write your code for Task 1\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('house_sales.csv')\ndf['city'].unique()\nhouse_sales.replace(['--', 'missing', 'N/A', 'na', 'NA'], np.nan, inplace=True)\nmissing_city = house_sales['city'].isnull().sum()\nprint(missing_city)\n\nimport pandas as pd\n\n# Muat data dari file CSV\nhouse_sales = pd.read_csv('house_sales.csv')\n\n# 1. Mengganti format nilai hilang yang berbeda di kolom 'city'\n# Mengganti tanda \"-\" dan \"missing\" dengan NaN, kemudian mengganti NaN dengan 'Unknown'\nhouse_sales['city'] = house_sales['city'].replace(['-', 'missing'], pd.NA)\nhouse_sales['city'].fillna('Unknown', inplace=True)\nvalid_cities = ['Silvertown', 'Riverford', 'Teasdale', 'Poppleton']\nhouse_sales['city'] = house_sales['city'].apply(lambda x: x if x in valid_cities else 'Unknown')\n\n# 2. Menghapus baris dengan nilai hilang di kolom 'sale_price'\nhouse_sales.dropna(subset=['sale_price'], inplace=True)\n\n# 3. Mengganti format nilai hilang yang berbeda di kolom 'sale_date'\n# Mengganti tanda \"-\" dan \"missing\" dengan NaN, kemudian mengganti NaN dengan '2023-01-01'\nhouse_sales['sale_date'] = house_sales['sale_date'].replace(['-', 'missing'], pd.NA)\nhouse_sales['sale_date'].fillna('2023-01-01', inplace=True)\n\n# 4. Mengganti nilai hilang di kolom 'months_listed' dengan rata-rata bulat ke satu desimal\nhouse_sales['months_listed'] = house_sales['months_listed'].replace(['-', 'missing'], pd.NA)\nhouse_sales['months_listed'] = pd.to_numeric(house_sales['months_listed'], errors='coerce')\nmean_months_listed = round(house_sales['months_listed'].mean(), 1)\nhouse_sales['months_listed'].fillna(mean_months_listed, inplace=True)\n\n# 5. Mengganti nilai hilang di kolom 'bedrooms' dengan rata-rata dibulatkan ke bilangan bulat terdekat\nhouse_sales['bedrooms'] = house_sales['bedrooms'].replace(['-', 'missing'], pd.NA)\nhouse_sales['bedrooms'] = pd.to_numeric(house_sales['bedrooms'], errors='coerce')\nmean_bedrooms = round(house_sales['bedrooms'].mean())\nhouse_sales['bedrooms'].fillna(mean_bedrooms, inplace=True)\n\n# 6 Define valid house types and their mappings\nstandard_house_types = {\n    'Terraced': ['Terraced', 'Terr.'],\n    'Semi-detached': ['Semi-detached', 'Semi'],\n    'Detached': ['Detached', 'Det.']\n}\n\n# Create a reverse mapping for convenience\nhouse_type_mapping = {value: key for key, values in standard_house_types.items() for value in values}\n\n# Apply the mapping to standardize the 'house_type' column\nhouse_sales['house_type'] = house_sales['house_type'].map(house_type_mapping).fillna('Unknown')\n\n# 7. Membersihkan kolom 'area' dari satuan \"sq.m.\" dan mengganti nilai hilang\nhouse_sales['area'] = house_sales['area'].replace(['-', 'missing'], pd.NA)\nhouse_sales['area'] = house_sales['area'].astype(str).str.replace(' sq.m.', '', regex=False).str.strip()\nhouse_sales['area'] = pd.to_numeric(house_sales['area'], errors='coerce')\nmean_area = round(house_sales['area'].mean(), 1)\nhouse_sales['area'].fillna(mean_area, inplace=True)\n\n# 1. Mengonversi kolom 'sale_date' menjadi tipe data datetime\nhouse_sales['sale_date'] = pd.to_datetime(house_sales['sale_date'], errors='coerce')\n\n# 2. Mengonversi kolom 'area' dari string ke float setelah menghapus satuan\nhouse_sales['area'] = house_sales['area'].astype(str).str.replace(' sq.m.', '', regex=False).str.strip()\nhouse_sales['area'] = pd.to_numeric(house_sales['area'], errors='coerce')\n\n# 3. Mengonversi kolom 'bedrooms' dan 'months_listed' menjadi integer dan float jika perlu\nhouse_sales['bedrooms'] = house_sales['bedrooms'].astype(int)\nhouse_sales['months_listed'] = house_sales['months_listed'].astype(float)\n\n# 4. Mengonversi kolom 'city' dan 'house_type' menjadi string jika belum\nhouse_sales['city'] = house_sales['city'].astype(str)\nhouse_sales['house_type'] = house_sales['house_type'].astype(str)\n\n# Simpan DataFrame yang bersih sebagai 'clean_data'\nclean_data = house_sales\n\n# Tampilkan DataFrame yang bersih\nprint(clean_data.head())\nprint(clean_data['house_id'].dtypes)\n\n\n# Use this cell to write your code for Task 3\nimport pandas as pd\n\n# Muat data dari file CSV\nhouse_sales = pd.read_csv('house_sales.csv')\n\n# 1. Mengganti nilai hilang di kolom 'sale_price' dengan penghapusan baris jika perlu\nhouse_sales.dropna(subset=['sale_price'], inplace=True)\n\n# 2. Mengelompokkan data berdasarkan 'bedrooms' dan menghitung rata-rata serta variansi harga jual\nprice_by_rooms = house_sales.groupby('bedrooms').agg(\n    avg_price=('sale_price', 'mean'),\n    var_price=('sale_price', 'var')\n).reset_index()\n\n# 3. Membulatkan hasil ke satu desimal\nprice_by_rooms['avg_price'] = price_by_rooms['avg_price'].round(1)\nprice_by_rooms['var_price'] = price_by_rooms['var_price'].round(1)\n\n# Tampilkan DataFrame\nprint(price_by_rooms)\n\n# Simpan DataFrame jika perlu\n# price_by_rooms.to_csv('price_by_rooms.csv', index=False)\n\n# Use this cell to write your code for Task 4\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n\n# 1. Load the data\ntrain_df = pd.read_csv('train.csv')\nvalidation_df = pd.read_csv('validation.csv')\n\n# 2. Preprocess the data\n# Drop columns not needed for prediction\nX_train = train_df.drop(columns=['house_id', 'sale_price', 'sale_date'])\ny_train = train_df['sale_price']\n\nX_validation = validation_df.drop(columns=['house_id', 'sale_date'])\n\n# Define categorical and numerical features\ncategorical_features = ['city', 'house_type']\nnumerical_features = ['months_listed', 'bedrooms', 'area']\n\n# Preprocess the data: handle missing values and encode categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='mean'), numerical_features),\n        ('cat', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='most_frequent')), \n            ('onehot', OneHotEncoder(handle_unknown='ignore'))]), \n        categorical_features)\n    ])\n\n# Define the model\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict the sale prices for the validation data\nvalidation_predictions = model.predict(X_validation)\n\n# Prepare the result dataframe\nbase_result = pd.DataFrame({\n    'house_id': validation_df['house_id'],\n    'price': validation_predictions\n})\n\n# Tampilkan DataFrame\nprint(base_result.head())\n\n# Simpan DataFrame jika perlu\n# base_result.to_csv('base_result.csv', index=False)\n\n\n# Use this cell to write your code for Task 5\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the data train data validation\ntrain_df = pd.read_csv('train.csv')\nvalidation_df = pd.read_csv('validation.csv')\n\n# Define the features and target variable for training\nX_train = train_df.drop(columns=['house_id', 'sale_price', 'sale_date'])\ny_train = train_df['sale_price']\n\n# Define the features for validation\nX_validation = validation_df.drop(columns=['house_id', 'sale_date'])\n\n# Preprocess the data: one-hot encode categorical variables\ncategorical_features = ['city', 'house_type']\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],\n    remainder='passthrough')\n\n# Define the model\nrf = RandomForestRegressor(random_state=42)\n\n# Define the parameter grid for Grid Search\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt']\n}\n\n# Grid Search with 5-fold cross-validation\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n\n# Define the model pipeline\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('grid_search', grid_search)\n])\n\n# Train the model\nmodel_pipeline.fit(X_train, y_train)\n\n# Predict the sale prices for the validation data\nvalidation_predictions = model_pipeline.predict(X_validation)\n\n# Prepare the result dataframe\ncompare_result = pd.DataFrame({\n    'house_id': validation_df['house_id'],\n    'price': validation_predictions\n})\n\ncompare_result.head()\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":131,"lastExecutedAt":1722607375140,"lastExecutedByKernel":"c315666a-c050-4bf0-807b-ac7c908f2042","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Use this cell to write your code for Task 1\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('house_sales.csv')\ndf['city'].unique()\nhouse_sales.replace(['--', 'missing', 'N/A', 'na', 'NA'], np.nan, inplace=True)\nmissing_city = house_sales['city'].isnull().sum()\nprint(missing_city)\n\nimport pandas as pd\n\n# Muat data dari file CSV\nhouse_sales = pd.read_csv('house_sales.csv')\n\n# 1. Mengganti format nilai hilang yang berbeda di kolom 'city'\n# Mengganti tanda \"-\" dan \"missing\" dengan NaN, kemudian mengganti NaN dengan 'Unknown'\nhouse_sales['city'] = house_sales['city'].replace(['-', 'missing'], pd.NA)\nhouse_sales['city'].fillna('Unknown', inplace=True)\nvalid_cities = ['Silvertown', 'Riverford', 'Teasdale', 'Poppleton']\nhouse_sales['city'] = house_sales['city'].apply(lambda x: x if x in valid_cities else 'Unknown')\n\n# 2. Menghapus baris dengan nilai hilang di kolom 'sale_price'\nhouse_sales.dropna(subset=['sale_price'], inplace=True)\n\n# 3. Mengganti format nilai hilang yang berbeda di kolom 'sale_date'\n# Mengganti tanda \"-\" dan \"missing\" dengan NaN, kemudian mengganti NaN dengan '2023-01-01'\nhouse_sales['sale_date'] = house_sales['sale_date'].replace(['-', 'missing'], pd.NA)\nhouse_sales['sale_date'].fillna('2023-01-01', inplace=True)\n\n# 4. Mengganti nilai hilang di kolom 'months_listed' dengan rata-rata bulat ke satu desimal\nhouse_sales['months_listed'] = house_sales['months_listed'].replace(['-', 'missing'], pd.NA)\nhouse_sales['months_listed'] = pd.to_numeric(house_sales['months_listed'], errors='coerce')\nmean_months_listed = round(house_sales['months_listed'].mean(), 1)\nhouse_sales['months_listed'].fillna(mean_months_listed, inplace=True)\n\n# 5. Mengganti nilai hilang di kolom 'bedrooms' dengan rata-rata dibulatkan ke bilangan bulat terdekat\nhouse_sales['bedrooms'] = house_sales['bedrooms'].replace(['-', 'missing'], pd.NA)\nhouse_sales['bedrooms'] = pd.to_numeric(house_sales['bedrooms'], errors='coerce')\nmean_bedrooms = round(house_sales['bedrooms'].mean())\nhouse_sales['bedrooms'].fillna(mean_bedrooms, inplace=True)\n\n# 6 Define valid house types and their mappings\nstandard_house_types = {\n    'Terraced': ['Terraced', 'Terr.'],\n    'Semi-detached': ['Semi-detached', 'Semi'],\n    'Detached': ['Detached', 'Det.']\n}\n\n# Create a reverse mapping for convenience\nhouse_type_mapping = {value: key for key, values in standard_house_types.items() for value in values}\n\n# Apply the mapping to standardize the 'house_type' column\nhouse_sales['house_type'] = house_sales['house_type'].map(house_type_mapping).fillna('Unknown')\n\n# 7. Membersihkan kolom 'area' dari satuan \"sq.m.\" dan mengganti nilai hilang\nhouse_sales['area'] = house_sales['area'].replace(['-', 'missing'], pd.NA)\nhouse_sales['area'] = house_sales['area'].astype(str).str.replace(' sq.m.', '', regex=False).str.strip()\nhouse_sales['area'] = pd.to_numeric(house_sales['area'], errors='coerce')\nmean_area = round(house_sales['area'].mean(), 1)\nhouse_sales['area'].fillna(mean_area, inplace=True)\n\n# 1. Mengonversi kolom 'sale_date' menjadi tipe data datetime\nhouse_sales['sale_date'] = pd.to_datetime(house_sales['sale_date'], errors='coerce')\n\n# 2. Mengonversi kolom 'area' dari string ke float setelah menghapus satuan\nhouse_sales['area'] = house_sales['area'].astype(str).str.replace(' sq.m.', '', regex=False).str.strip()\nhouse_sales['area'] = pd.to_numeric(house_sales['area'], errors='coerce')\n\n# 3. Mengonversi kolom 'bedrooms' dan 'months_listed' menjadi integer dan float jika perlu\nhouse_sales['bedrooms'] = house_sales['bedrooms'].astype(int)\nhouse_sales['months_listed'] = house_sales['months_listed'].astype(float)\n\n# 4. Mengonversi kolom 'city' dan 'house_type' menjadi string jika belum\nhouse_sales['city'] = house_sales['city'].astype(str)\nhouse_sales['house_type'] = house_sales['house_type'].astype(str)\n\n# Simpan DataFrame yang bersih sebagai 'clean_data'\nclean_data = house_sales\n\n# Tampilkan DataFrame yang bersih\nprint(clean_data.head())\nprint(clean_data['house_id'].dtypes)\n"},"cell_type":"code","id":"2127401e-81ac-4638-bc22-0bebbb532040","outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhouse_sales.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      7\u001b[0m house_sales\u001b[38;5;241m.\u001b[39mreplace([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mna\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'house_sales.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'house_sales.csv'"}],"execution_count":null}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}